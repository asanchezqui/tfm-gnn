{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with LFR and testing over real networks 2\n",
    "\n",
    "This notebook contains some code that can be used for performing the experiments showed on section 3.3.2 in which the modesl are trained using LFR networks and tested over real networks\n",
    "\n",
    "A set of 5 LFR, 5 SF, 50 LFR and 50 SF graph sets are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkit import *\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx2nkit(g_nx):\n",
    "    \n",
    "    node_num = g_nx.number_of_nodes()\n",
    "    g_nkit = Graph(directed=True)\n",
    "    \n",
    "    for i in range(node_num):\n",
    "        g_nkit.addNode()\n",
    "    \n",
    "    for e1,e2 in g_nx.edges():\n",
    "        g_nkit.addEdge(e1,e2)\n",
    "        \n",
    "    assert g_nx.number_of_nodes()==g_nkit.numberOfNodes(),\"Number of nodes not matching\"\n",
    "    assert g_nx.number_of_edges()==g_nkit.numberOfEdges(),\"Number of edges not matching\"\n",
    "        \n",
    "    return g_nkit\n",
    "\n",
    "\n",
    "def cal_exact_bet(g_nkit):\n",
    "\n",
    "    exact_bet = centrality.Betweenness(g_nkit,normalized=True).run().ranking()\n",
    "    exact_bet_dict = dict()\n",
    "    for j in exact_bet:\n",
    "        exact_bet_dict[j[0]] = j[1]\n",
    "    return exact_bet_dict\n",
    "\n",
    "\n",
    "def cal_exact_degree(g_nkit):\n",
    "\n",
    "    exact_deg = centrality.DegreeCentrality(g_nkit,normalized=False).run().ranking()\n",
    "    exact_deg_dict = dict()\n",
    "    for j in exact_deg:\n",
    "        exact_deg_dict[j[0]] = j[1]\n",
    "    return exact_deg_dict\n",
    "\n",
    "    \n",
    "def generate_bet_LFR_data(num_nodes, num_of_graphs,output_path):\n",
    "    \n",
    "    list_bet_data = list()\n",
    "\n",
    "    for i in range(num_of_graphs):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(f\"Graph index:{i+1}/{num_of_graphs}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "                g_nx = LFR_benchmark_graph(n=num_nodes,tau1=3,tau2=1.5,mu=0.05,average_degree=6,min_community=20)\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        print(\"removing isolates\")\n",
    "        \n",
    "        if nx.number_of_isolates(g_nx)>0:\n",
    "            g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "        \n",
    "        g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "        g_nkit = nx2nkit(g_nx)\n",
    "        bet_dict = cal_exact_bet(g_nkit)\n",
    "        deg_dict = cal_exact_degree(g_nkit)\n",
    "        list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        with open(output_path,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sets of 5 and 50 LFR are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_graphs = 5\n",
    "num_nodes = 10000\n",
    "output_path = f\"graphs/LFR_{num_of_graphs}_graphs_{num_nodes}_nodes.pickle\"\n",
    "\n",
    "generate_bet_LFR_data(num_nodes, num_of_graphs,output_path)\n",
    "\n",
    "num_of_graphs = 50\n",
    "num_nodes = 10000\n",
    "output_path = f\"graphs/LFR_{num_of_graphs}_graphs_{num_nodes}_nodes.pickle\"\n",
    "\n",
    "generate_bet_LFR_data(num_nodes, num_of_graphs,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./functions\")\n",
    "\n",
    "from utils import *\n",
    "from model_bet import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sets of 5 and 50 SF are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 10000,\n",
    "    \"max_nodes\": 10001,\n",
    "    \"num_of_graphs\": 5,\n",
    "    \"graph_types\": [\"SF\"]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    list_bet_data = list()\n",
    "    print(\"Generating graphs and calculating centralities...\")\n",
    "    for i in range(param['num_of_graphs']):\n",
    "        print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "        g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "        \n",
    "        if nx.number_of_isolates(g_nx)>0:\n",
    "            #print(\"Graph has isolates.\")\n",
    "            g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "        \n",
    "        g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "        \n",
    "        g_nkit = nx2nkit(g_nx)\n",
    "        bet_dict = cal_exact_bet(g_nkit)\n",
    "        deg_dict = cal_exact_degree(g_nkit)\n",
    "        list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "    fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['min_nodes']}_nodes.pickle\"    \n",
    "\n",
    "    with open(fname_bet,\"wb\") as fopen:\n",
    "        pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 10000,\n",
    "    \"max_nodes\": 10001,\n",
    "    \"num_of_graphs\": 50,\n",
    "    \"graph_types\": [\"SF\"]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    list_bet_data = list()\n",
    "    print(\"Generating graphs and calculating centralities...\")\n",
    "    for i in range(param['num_of_graphs']):\n",
    "        print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "        g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "        \n",
    "        if nx.number_of_isolates(g_nx)>0:\n",
    "            #print(\"Graph has isolates.\")\n",
    "            g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "        \n",
    "        g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "        \n",
    "        g_nkit = nx2nkit(g_nx)\n",
    "        bet_dict = cal_exact_bet(g_nkit)\n",
    "        deg_dict = cal_exact_degree(g_nkit)\n",
    "        list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "    fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['min_nodes']}_nodes.pickle\"    \n",
    "\n",
    "    with open(fname_bet,\"wb\") as fopen:\n",
    "        pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SF and LFR training datasets for testing over real networks are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "param = {\n",
    "    \"graph_types\": [\"LFR\",\"SF\"],\n",
    "    \"set_graphs\": [5,50],\n",
    "    \"num_train_set_graph\": [5,50],\n",
    "    \"nodes\": 10000,\n",
    "    \"size\" : [10000],#[10000,100000,300000,900000],\n",
    "    \"num_test\" : 0,\n",
    "    \"num_copies\": [1]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "    for idx,set_graph in enumerate(param[\"set_graphs\"]):\n",
    "\n",
    "        with open(f\"./graphs/{graph_type}_{set_graph}_graphs_{param['nodes']}_nodes.pickle\",\"rb\") as fopen:\n",
    "            list_data = pickle.load(fopen)\n",
    "\n",
    "        num_graph = len(list_data)\n",
    "        num_train = param[\"num_train_set_graph\"][idx]\n",
    "        assert num_train+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "\n",
    "        for size in param[\"size\"]:\n",
    "            for c in param[\"num_copies\"]:\n",
    "                #For training split\n",
    "                if num_train > 0:\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:num_train],num_copies = c,adj_size=size)\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_type}_{num_train}_graphs_{param['nodes']}_nodes_{c}_copies_{size}_size.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving models with the generated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = {\n",
    "    \"graph_types\": [\"LFR\",\"SF\"],\n",
    "    \"num_train_graphs\": [5,50],\n",
    "    \"nodes\": 10000,\n",
    "    \"size\" : [10000],#[10000,100000,300000,900000],\n",
    "    \"num_copies\": [1],#10,20,40],\n",
    "    \"model_seeds\": [j for j in range(15)],\n",
    "    \"num_epochs\": 10\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "    for num_train_graphs in param[\"num_train_graphs\"]:\n",
    "        for size in param[\"size\"]:\n",
    "            for c in param[\"num_copies\"]:\n",
    "\n",
    "                data_train = f\"{graph_type}_{num_train_graphs}_graphs_{param['nodes']}_nodes_{c}_copies_{size}_size.pickle\"    \n",
    "\n",
    "                #Load training data\n",
    "                print(f\"Loading data...\")\n",
    "                with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "                    list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train, deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "\n",
    "                for seed in param[\"model_seeds\"]:\n",
    "\n",
    "                    #Model parameters\n",
    "                    hidden = 20\n",
    "                    \n",
    "                    torch.manual_seed(seed)\n",
    "\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.to(device)\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                    for e in range(num_epoch):\n",
    "                        print(f\"{graph_type}_{num_train_graphs}_{c}_copies_{size}_size_{e}_epoch_{seed}_seed_{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "                        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,size)\n",
    "                        \n",
    "                        saving_path = f\"./models/{graph_type}/{graph_type}_{num_train_graphs}_graphs_{param['nodes']}_nodes_{c}_copies_{size}_size_{seed}_seed_{e}_epoch\"\n",
    "                        torch.save(model.state_dict(), saving_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the generated models and testing over real networks when using 5 training SF and LFR graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"graphs\": ['1-wiki-Vote'],#'2-soc-Epinions'],\n",
    "    \"sizes\" : [10000],#100000],\n",
    "    \"train_graph_nodes\": 10000,\n",
    "    \"copies\": [1],#10,20,40],\n",
    "    \"seeds\": [j for j in range(15)],\n",
    "    \"epochs\": [j for j in range(10)]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "Results = {j:{'LFR': {}, 'SF': {}} for j in param[\"graphs\"]}\n",
    "\n",
    "for i in range(len(param[\"graphs\"])):\n",
    "\n",
    "    g = param[\"graphs\"][i]\n",
    "    size = param[\"sizes\"][i]\n",
    "    data_test = f'{g}_{size}_size.pickle'\n",
    "\n",
    "    #Load test data\n",
    "    with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "    Results[g]['LFR'][f\"{size}_size\"] = {'test_graph': data_test, 'real': [], 'deg': []}\n",
    "    Results[g]['SF'][f\"{size}_size\"] = {'test_graph': data_test, 'real': [], 'deg': []}\n",
    "\n",
    "    for c in param[\"copies\"]:\n",
    "    \n",
    "        LFR_data_train = f\"LFR_5_graphs_{param['train_graph_nodes']}_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        SF_data_train = f\"SF_5_graphs_{param['train_graph_nodes']}_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        Results[g]['LFR'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : LFR_data_train,'pred':{}}\n",
    "        Results[g]['SF'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : SF_data_train,'pred':{}}\n",
    "        \n",
    "        for epoch in param[\"epochs\"]:\n",
    "            \n",
    "            Results[g]['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "            Results[g]['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "        \n",
    "            for seed in param[\"seeds\"]:\n",
    "                    \n",
    "                    data_train = LFR_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    print(model_path)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/LFR/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results[g]['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f'{seed}_seed'] = {'pred':r['pred'],'kt':r['kt']}\n",
    "\n",
    "                    if len(Results[g]['LFR'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results[g]['LFR'][f\"{size}_size\"]['real'] = r['true']\n",
    "                        Results[g]['LFR'][f\"{size}_size\"]['deg'] = r['deg']\n",
    "\n",
    "                    with open(f\"./outputs/LFR_SF_real_performance_5_traingraphs_{len(param['epochs'])}_epochs_{len(param['seeds'])}_seeds.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n",
    "\n",
    "                    data_train = SF_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    print(model_path)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/SF/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results[g]['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f'{seed}_seed'] = {'pred':r['pred'],'kt':r['kt']}\n",
    "\n",
    "                    if len(Results[g]['SF'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results[g]['SF'][f\"{size}_size\"]['real'] = r['true']\n",
    "                        Results[g]['SF'][f\"{size}_size\"]['deg'] = r['deg']\n",
    "\n",
    "                    with open(f\"./outputs/LFR_SF_real_performance_5_traingraphs_{len(param['epochs'])}_epochs_{len(param['seeds'])}_seeds.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the generated models and testing over real networks when using 50 training SF and LFR graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"graphs\": ['1-wiki-Vote'],#'2-soc-Epinions'],\n",
    "    \"sizes\" : [10000],#100000],\n",
    "    \"train_graph_nodes\": 10000,\n",
    "    \"copies\": [1],#10,20,40],\n",
    "    \"seeds\": [j for j in range(15)],\n",
    "    \"epochs\": [j for j in range(10)]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "Results = {j:{'LFR': {}, 'SF': {}} for j in param[\"graphs\"]}\n",
    "\n",
    "for i in range(len(param[\"graphs\"])):\n",
    "\n",
    "    g = param[\"graphs\"][i]\n",
    "    size = param[\"sizes\"][i]\n",
    "    data_test = f'{g}_{size}_size.pickle'\n",
    "\n",
    "    #Load test data\n",
    "    with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "    Results[g]['LFR'][f\"{size}_size\"] = {'test_graph': data_test, 'real': [], 'deg': []}\n",
    "    Results[g]['SF'][f\"{size}_size\"] = {'test_graph': data_test, 'real': [], 'deg': []}\n",
    "\n",
    "    for c in param[\"copies\"]:\n",
    "    \n",
    "        LFR_data_train = f\"LFR_50_graphs_{param['train_graph_nodes']}_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        SF_data_train = f\"SF_50_graphs_{param['train_graph_nodes']}_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        Results[g]['LFR'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : LFR_data_train,'pred':{}}\n",
    "        Results[g]['SF'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : SF_data_train,'pred':{}}\n",
    "        \n",
    "        for epoch in param[\"epochs\"]:\n",
    "            \n",
    "            Results[g]['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "            Results[g]['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "        \n",
    "            for seed in param[\"seeds\"]:\n",
    "                    \n",
    "                    data_train = LFR_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    print(model_path)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/LFR/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results[g]['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f'{seed}_seed'] = {'pred':r['pred'],'kt':r['kt']}\n",
    "\n",
    "                    if len(Results[g]['LFR'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results[g]['LFR'][f\"{size}_size\"]['real'] = r['true']\n",
    "                        Results[g]['LFR'][f\"{size}_size\"]['deg'] = r['deg']\n",
    "\n",
    "                    with open(f\"./outputs/LFR_SF_real_performance_50_traingraphs_{len(param['epochs'])}_epochs_{len(param['seeds'])}_seeds.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n",
    "\n",
    "                    data_train = SF_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    print(model_path)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/SF/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results[g]['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f'{seed}_seed'] = {'pred':r['pred'],'kt':r['kt']}\n",
    "\n",
    "                    if len(Results[g]['SF'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results[g]['SF'][f\"{size}_size\"]['real'] = r['true']\n",
    "                        Results[g]['SF'][f\"{size}_size\"]['deg'] = r['deg']\n",
    "\n",
    "                    with open(f\"./outputs/LFR_SF_real_performance_50_traingraphs_{len(param['epochs'])}_epochs_{len(param['seeds'])}_seeds.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836981034a4078c9f81aa3bbf2605e6a2991c189feb0614c725b1b8d5991d7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
