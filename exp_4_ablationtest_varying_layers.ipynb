{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation experiments 1\n",
    "This notebook contains some code that can be used for performing the ablation experiments showed on section 3.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./functions\")\n",
    "\n",
    "from utils import *\n",
    "from model_bet_varying_layers import *\n",
    "from layer import *\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the model when varying  the  number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { \n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \"num_train\" : 5,\n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"split_seeds\": [10],\n",
    "    'model_seed' : 15,\n",
    "    \"num_epochs\": 15,    \n",
    "    \"num_test\" : 10,\n",
    "}\n",
    "\n",
    "\n",
    "results = {'g': [], 'layers':[], 'epoch':[], 'kt':[], 'std':[]}\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "                for num_layers in range(1,8):\n",
    "\n",
    "                    data_path_test = f\"{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "                    #Load test data\n",
    "                    with open(\"./data_splits/test/\"+data_path_test,\"rb\") as fopen:\n",
    "                        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)        \n",
    "\n",
    "                    data_path_train = f\"{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "\n",
    "                    #Load training data\n",
    "                    print(f\"Loading data...\")\n",
    "                    with open(\"./data_splits/train/\"+data_path_train,\"rb\") as fopen:\n",
    "                        list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                    model_size = bc_mat_train.shape[0]\n",
    "\n",
    "                    #Get adjacency matrices from graphs\n",
    "                    print(f\"Graphs to adjacency conversion.\")\n",
    "\n",
    "                    list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,model_size)\n",
    "                    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,model_size)\n",
    "\n",
    "\n",
    "                    #Model parameters\n",
    "                    torch.manual_seed(param[\"model_seed\"])\n",
    "\n",
    "                    hidden = 20\n",
    "\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                    print(f\"GNN_Bet{num_layers}\")\n",
    "                    exec(f'model = GNN_Bet{num_layers}(ninput=model_size,nhid=hidden,dropout=0.6)')\n",
    "                    model.to(device)\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                    print(\"Training\")\n",
    "                    print(f\"Total Number of epoches: {num_epoch}\")\n",
    "                    for e in range(num_epoch):\n",
    "                        print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model=model,device=device,optimizer=optimizer,size=model_size)\n",
    "\n",
    "                        #to check test loss while training\n",
    "                        with torch.no_grad():\n",
    "                            r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model=model,device=device,size=model_size)\n",
    "                        \n",
    "                            results['g'].append(data_path_train)\n",
    "                            results['layers'].append(num_layers)\n",
    "                            results['epoch'].append(e)\n",
    "                            results['kt'].append(r[\"kt\"])\n",
    "                            results['std'].append(r[\"std\"])\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(results)\n",
    "                        df.to_csv(\"./outputs/output_ablationtest_varying_layers_given.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836981034a4078c9f81aa3bbf2605e6a2991c189feb0614c725b1b8d5991d7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
