{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic graphs performance\n",
    "This notebook contains some code that can be used for performing the experiments showed on section 3.2.1 for synthetic graphs\n",
    "\n",
    "\n",
    "The model is trained and tested over the different sets of synthetic graphs considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./functions\")\n",
    "\n",
    "from utils import *\n",
    "from model_bet import *\n",
    "from layer import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General synthetic performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The different graphs are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 500,#5000,\n",
    "    \"max_nodes\": 1000,#10000,\n",
    "    \"num_of_graphs\": 15,\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#,\"GRP\"],\n",
    "    \"generation_seeds\": [10]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    for seed in param[\"generation_seeds\"]:\n",
    "        \n",
    "        random.seed(seed)\n",
    "        print(f\"Generating {param['num_of_graphs']} {graph_type} graphs\")\n",
    "        list_bet_data = list()\n",
    "        for i in range(param['num_of_graphs']):\n",
    "            print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "            g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "            \n",
    "            if nx.number_of_isolates(g_nx)>0:\n",
    "                g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "                g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "\n",
    "            g_nkit = nx2nkit(g_nx)\n",
    "            bet_dict = cal_exact_bet(g_nkit)\n",
    "            deg_dict = cal_exact_degree(g_nkit)\n",
    "            list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['max_nodes']}_{param['min_nodes']}_nodes_{seed}_genseed.pickle\"    \n",
    "\n",
    "        with open(fname_bet,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The corresponding datasets are created (data split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model's performance is analysed when training and testing over synthetic graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import pandas as pd\n",
    "\n",
    "param = {\n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                                                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "\n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"generation_seed\":[],\n",
    "            \"splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for splitseed in param[\"split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for num_copies in param[\"num_copies\"]:\n",
    "\n",
    "                train_file = f\"{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "                #Load training data\n",
    "                print(f\"Loading data...\")\n",
    "                with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                    list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                model_size = bc_mat_train.shape[0]\n",
    "                assert model_size == param['adj_size']\n",
    "                \n",
    "                list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "                \n",
    "                for model_seed in param[\"model_seeds\"]:\n",
    "                    #Model parameters\n",
    "\n",
    "                    torch.manual_seed(model_seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                    model.to(device)\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                    print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                    for e in range(num_epoch):\n",
    "                        print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                        #to check test loss while training\n",
    "                        with torch.no_grad():\n",
    "                            r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                        Results[\"gtype_train\"].append(train_file)\n",
    "                        Results[\"generation_seed\"].append(genseed)\n",
    "                        Results[\"splilt_seed\"].append(splitseed)\n",
    "                        Results[\"copies\"].append(num_copies)\n",
    "                        Results[\"adj_size\"].append(model_size)\n",
    "                        Results[\"model_seed\"].append(model_seed)\n",
    "                        Results[\"epochs\"].append(e)\n",
    "                        Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                        Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(Results)\n",
    "                        #df.to_csv(\"output_synthetic_graphs_performance.csv\")\n",
    "                        df.to_csv(\"./outputs/synthetic_graphs_performance_exp1.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the replication parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some datasets are created varying replication parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \n",
    "    \"num_copies\": [1,2,10,20,40],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "\n",
    "        with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "            list_data = pickle.load(fopen)\n",
    "\n",
    "        num_graph = len(list_data)\n",
    "        assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "    \n",
    "        for splitseed in param[\"split_seeds\"]:\n",
    "            \n",
    "            random.seed(splitseed)\n",
    "\n",
    "            #For test split\n",
    "            if param[\"num_test\"] > 0:\n",
    "                list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                    pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "                    \n",
    "            for num_copies in param[\"num_copies\"]:\n",
    "                \n",
    "                random.seed(splitseed)\n",
    "                \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    \n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The performance of the  model is tested when varying the replication parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import pandas as pd\n",
    "\n",
    "param = {\n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                                                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "\n",
    "    \"num_copies\": [1,2,10,20,40],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"generation_seed\":[],\n",
    "            \"splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for splitseed in param[\"split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for num_copies in param[\"num_copies\"]:\n",
    "\n",
    "                train_file = f\"{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "                #Load training data\n",
    "                print(f\"Loading data...\")\n",
    "                with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                    list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                model_size = bc_mat_train.shape[0]\n",
    "                assert model_size == param['adj_size']\n",
    "                \n",
    "                list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "                \n",
    "                for model_seed in param[\"model_seeds\"]:\n",
    "                    #Model parameters\n",
    "\n",
    "                    torch.manual_seed(model_seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                    model.to(device)\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                    print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                    for e in range(num_epoch):\n",
    "                        print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                        #to check test loss while training\n",
    "                        with torch.no_grad():\n",
    "                            r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                        Results[\"gtype_train\"].append(train_file)\n",
    "                        Results[\"generation_seed\"].append(genseed)\n",
    "                        Results[\"splilt_seed\"].append(splitseed)\n",
    "                        Results[\"copies\"].append(num_copies)\n",
    "                        Results[\"adj_size\"].append(model_size)\n",
    "                        Results[\"model_seed\"].append(model_seed)\n",
    "                        Results[\"epochs\"].append(e)\n",
    "                        Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                        Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(Results)\n",
    "                        #df.to_csv(\"output_synthetic_graphs_performance.csv\")\n",
    "                        df.to_csv(\"./outputs/synthetic_graphs_performance_varying_replication_parameter.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering variations on replication and graph generation random seeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A set of 10 synthetic graphs are created for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 500,#5000,\n",
    "    \"max_nodes\": 1000,#10000,\n",
    "    \"num_of_graphs\": 10,\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#,\"GRP\"],\n",
    "    \"generation_seeds\": [10]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    for seed in param[\"generation_seeds\"]:\n",
    "        \n",
    "        random.seed(seed)\n",
    "        print(f\"Generating {param['num_of_graphs']} {graph_type} graphs\")\n",
    "        list_bet_data = list()\n",
    "        for i in range(param['num_of_graphs']):\n",
    "            print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "            g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "            \n",
    "            if nx.number_of_isolates(g_nx)>0:\n",
    "                g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "                g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "\n",
    "            g_nkit = nx2nkit(g_nx)\n",
    "            bet_dict = cal_exact_bet(g_nkit)\n",
    "            deg_dict = cal_exact_degree(g_nkit)\n",
    "            list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['max_nodes']}_{param['min_nodes']}_nodes_{seed}_genseed.pickle\"    \n",
    "\n",
    "        with open(fname_bet,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A set of 5 synthetic training graphs are created using different random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 500,#5000,\n",
    "    \"max_nodes\": 1000,#10000,\n",
    "    \"num_of_graphs\": 5,\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#,\"GRP\"],\n",
    "    \"generation_seeds\": [j for j in range(5)]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    for seed in param[\"generation_seeds\"]:\n",
    "        \n",
    "        random.seed(seed)\n",
    "        print(f\"Generating {param['num_of_graphs']} {graph_type} graphs\")\n",
    "        list_bet_data = list()\n",
    "        for i in range(param['num_of_graphs']):\n",
    "            print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "            g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "            \n",
    "            if nx.number_of_isolates(g_nx)>0:\n",
    "                g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "                g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "\n",
    "            g_nkit = nx2nkit(g_nx)\n",
    "            bet_dict = cal_exact_bet(g_nkit)\n",
    "            deg_dict = cal_exact_degree(g_nkit)\n",
    "            list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['max_nodes']}_{param['min_nodes']}_nodes_{seed}_genseed.pickle\"    \n",
    "\n",
    "        with open(fname_bet,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The splits for the test sets are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_10_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_10_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [0],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 0,\n",
    "    \"num_test\" : 10,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The splits for the training sets are created using different random seeds generation and fixed split seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_5_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_5_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [j for j in range(5)],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [0],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 0,\n",
    "    \n",
    "}\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The splits for the training sets are generated with different split seeds and a generation seed fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_5_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_5_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [0],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [j for j in range(5)],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 0,\n",
    "    \n",
    "}\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is  trained and tested considering the diferent  training and testing sets created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering variations on generation seeds\n",
    "\n",
    "param = {\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#[\"ER\",\"SF\",\"GRP\"],\n",
    "    \"graphs_sizes\": \"1000_500_nodes\",\n",
    "    \"test_generation_seeds\": [10],\n",
    "    \"train_generation_seeds\": [j for j in range(5)],\n",
    "    \"test_split_seeds\": [0],\n",
    "    \"train_split_seeds\": [0],\n",
    "    \"num_copies\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"train_generation_seed\": [],\n",
    "            \"train_splilt_seed\": [],\n",
    "            \"test_generation_seed\": [],\n",
    "            \"test_splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "    for testgenseed in param[\"test_generation_seeds\"]:\n",
    "        for testsplitseed in param[\"test_split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_type}_{param['num_test']}_graphs_{param['graphs_sizes']}_{testgenseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{testsplitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for traingenseed in param[\"train_generation_seeds\"]:\n",
    "                for trainsplitseed in param[\"train_split_seeds\"]:\n",
    "                    for num_copies in param[\"num_copies\"]:\n",
    "                        \n",
    "                        train_file = f\"{graph_type}_{param['num_train']}_graphs_{param['graphs_sizes']}_{traingenseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{trainsplitseed}_splitseed.pickle\"\n",
    "\n",
    "                        #Load training data\n",
    "                        print(f\"Loading data...\")\n",
    "                        with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "\n",
    "                        model_size = bc_mat_train.shape[0]\n",
    "                        assert model_size == param['adj_size']\n",
    "                        \n",
    "                        for model_seed in param[\"model_seeds\"]:\n",
    "                            #Model parameters\n",
    "\n",
    "                            torch.manual_seed(model_seed)\n",
    "                            \n",
    "                            hidden = 20\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                            model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                            model.to(device)\n",
    "\n",
    "                            optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                            num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                            print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                            for e in range(num_epoch):\n",
    "                                print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                                train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                                #to check test loss while training\n",
    "                                with torch.no_grad():\n",
    "                                    r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                                Results[\"gtype_train\"].append(train_file)\n",
    "                                Results[\"train_generation_seed\"].append(traingenseed)\n",
    "                                Results[\"train_splilt_seed\"].append(trainsplitseed)\n",
    "                                Results[\"test_generation_seed\"].append(testgenseed)\n",
    "                                Results[\"test_splilt_seed\"].append(testsplitseed)\n",
    "                                Results[\"copies\"].append(num_copies)\n",
    "                                Results[\"adj_size\"].append(model_size)\n",
    "                                Results[\"model_seed\"].append(model_seed)\n",
    "                                Results[\"epochs\"].append(e)\n",
    "                                Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                                Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                                df = pd.DataFrame.from_dict(Results)\n",
    "                                df.to_csv(\"./outputs/synthetic_graphs_performance_variating_generation_random_seeds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering variations on replication seeds\n",
    "\n",
    "param = {\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#[\"ER\",\"SF\",\"GRP\"],\n",
    "    \"graphs_sizes\": \"1000_500_nodes\",\n",
    "    \"test_generation_seeds\": [10],\n",
    "    \"train_generation_seeds\": [0],\n",
    "    \"test_split_seeds\": [0],\n",
    "    \"train_split_seeds\": [j for j in range(5)],\n",
    "    \"num_copies\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"train_generation_seed\": [],\n",
    "            \"train_splilt_seed\": [],\n",
    "            \"test_generation_seed\": [],\n",
    "            \"test_splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "    for testgenseed in param[\"test_generation_seeds\"]:\n",
    "        for testsplitseed in param[\"test_split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_type}_{param['num_test']}_graphs_{param['graphs_sizes']}_{testgenseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{testsplitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for traingenseed in param[\"train_generation_seeds\"]:\n",
    "                for trainsplitseed in param[\"train_split_seeds\"]:\n",
    "                    for num_copies in param[\"num_copies\"]:\n",
    "                        \n",
    "                        train_file = f\"{graph_type}_{param['num_train']}_graphs_{param['graphs_sizes']}_{traingenseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{trainsplitseed}_splitseed.pickle\"\n",
    "\n",
    "                        #Load training data\n",
    "                        print(f\"Loading data...\")\n",
    "                        with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "\n",
    "                        model_size = bc_mat_train.shape[0]\n",
    "                        assert model_size == param['adj_size']\n",
    "                        \n",
    "                        for model_seed in param[\"model_seeds\"]:\n",
    "                            #Model parameters\n",
    "\n",
    "                            torch.manual_seed(model_seed)\n",
    "                            \n",
    "                            hidden = 20\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                            model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                            model.to(device)\n",
    "\n",
    "                            optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                            num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                            print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                            for e in range(num_epoch):\n",
    "                                print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                                train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                                #to check test loss while training\n",
    "                                with torch.no_grad():\n",
    "                                    r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                                Results[\"gtype_train\"].append(train_file)\n",
    "                                Results[\"train_generation_seed\"].append(traingenseed)\n",
    "                                Results[\"train_splilt_seed\"].append(trainsplitseed)\n",
    "                                Results[\"test_generation_seed\"].append(testgenseed)\n",
    "                                Results[\"test_splilt_seed\"].append(testsplitseed)\n",
    "                                Results[\"copies\"].append(num_copies)\n",
    "                                Results[\"adj_size\"].append(model_size)\n",
    "                                Results[\"model_seed\"].append(model_seed)\n",
    "                                Results[\"epochs\"].append(e)\n",
    "                                Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                                Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                                df = pd.DataFrame.from_dict(Results)\n",
    "                                df.to_csv(\"./outputs/synthetic_graphs_performance_variating_replication_random_seeds.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836981034a4078c9f81aa3bbf2605e6a2991c189feb0614c725b1b8d5991d7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
